{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import glob\n",
    "import os.path as osp\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils import data as D\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import zipfile \n",
    "import torch\n",
    "import time\n",
    "import torchvision.transforms as T\n",
    "import random as rnd\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.transforms import functional as TF\n",
    "from torchvision.utils import make_grid,save_image\n",
    "from torchvision.datasets import ImageFolder,DatasetFolder\n",
    "from torch.utils.data import Dataset,DataLoader,Subset\n",
    "from torch.autograd import Variable\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from numpy.random import choice\n",
    "from numpy.random import seed as np_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def numericalSort(value):\n",
    "    numbers = re.compile(r'(\\d+)')\n",
    "    parts = numbers.split(value)\n",
    "    parts[1::2] = map(int, parts[1::2])\n",
    "    return parts\n",
    "\n",
    "bottleneck = 4000\n",
    "image_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(D.Dataset):\n",
    "    def __init__(self, path_mask, path_gt):\n",
    "        super(Data, self).__init__()\n",
    "        self.masked = []\n",
    "        self.gt = []\n",
    "        self.path_mask = path_mask\n",
    "        self.path_gt = path_gt\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "        files_mask = sorted(glob.glob(os.path.join(path_mask, '*.jpg')), key=numericalSort)[:15000]\n",
    "        files_gt = sorted(glob.glob(os.path.join(path_gt, '*.jpg')), key = numericalSort)[:15000]\n",
    "        for mask, gt in  zip(files_mask, files_gt):\n",
    "            self.masked.append(mask)\n",
    "            self.gt.append(gt)\n",
    "        self.len = len(self.masked)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img_masked = Image.open(self.masked[index])\n",
    "        img_gt = Image.open(self.gt[index])\n",
    "        return self.transform(img_masked), self.transform(img_gt)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_mask = '/home/sghosal/Project/Image_Inpainting/data_places_mask/'\n",
    "path_gt = '/home/sghosal/Project/Image_Inpainting/data_places_gt/'\n",
    "imgs = Data(path_mask, path_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m \u001b[43mD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ece285/lib/python3.9/site-packages/torch/utils/data/dataloader.py:277\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[0;32m--> 277\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ece285/lib/python3.9/site-packages/torch/utils/data/sampler.py:97\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement))\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue, but got num_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples))\n",
      "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "train_loader = D.DataLoader(imgs, batch_size = 32, shuffle = True, num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "\n",
    "        # input: image_size * image_size * 3; image_size = 256\n",
    "        self.encoder_layer_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3,out_channels=64,kernel_size=(4,4),stride=2,padding=1),\n",
    "            nn.LeakyReLU(0.2,in_place=True)\n",
    "        )\n",
    "        # input: 128 * 128 * 64\n",
    "        self.encoder_layer_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64,out_channels=64,kernel_size=(4,4),stride=2,padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2,in_place=True)\n",
    "        )\n",
    "        # input: 64 * 64 * 64\n",
    "        self.encoder_layer_3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64,out_channels=64,kernel_size=(4,4),stride=2,padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2,in_place=True)\n",
    "        )\n",
    "        # input: 32 * 32 * 64\n",
    "        self.encoder_layer_4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64,out_channels=256,kernel_size=(4,4),stride=2,padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2,in_place=True)\n",
    "        )\n",
    "        # input: 16 * 16 * 256\n",
    "        self.encoder_layer_5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256,out_channels=256,kernel_size=(4,4),stride=2,padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2,in_place=True)\n",
    "        )\n",
    "        # input: 8 * 8 * 256\n",
    "        self.encoder_layer_6 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256,out_channels=512,kernel_size=(4,4),stride=2,padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2,in_place=True)\n",
    "        )\n",
    "        # input: 4 * 4 * 512\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(512,bottleneck,kernel_size=(4,4)), \n",
    "            nn.BatchNorm2d(bottleneck),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # current state: 1 * 1 * bottleneck\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_layer_1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=bottleneck,out_channels=512,kernel_size=(4,4)),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Input: 4 * 4 * 512\n",
    "        self.decoder_layer_2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=512,out_channels=256,kernel_size=(4,4),stride=2,padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Input: 8 * 8 * 256\n",
    "        self.decoder_layer_3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=256,out_channels=128,kernel_size=(4,4),stride=2,padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Input: 16 * 16 * 128\n",
    "        self.decoder_layer_4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=128,out_channels=64,kernel_size=(4,4),stride=2,padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Input: 32 * 32 * 64\n",
    "        self.decoder_layer_5 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=64,out_channels=3,kernel_size=(4,4),stride=2,padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        # Output: 64 * 64 * 3\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.encoder_layer_1(x)\n",
    "        x = self.encoder_layer_2(x)\n",
    "        x = self.encoder_layer_3(x)\n",
    "        x = self.encoder_layer_4(x)\n",
    "        x = self.encoder_layer_5(x)\n",
    "        x = self.encoder_layer_6(x)\n",
    "        x = self.bottleneck(x)\n",
    "        x = self.decoder_layer_1(x)\n",
    "        x = self.decoder_layer_2(x)\n",
    "        x = self.decoder_layer_3(x)\n",
    "        x = self.decoder_layer_4(x)\n",
    "        x = self.decoder_layer_5(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # Input: 64 * 64 * 3\n",
    "        self.disc_layer_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3,out_channels=64,kernel_size=(4,4),stride=2,padding=1),\n",
    "            nn.LeakyReLU(0.2,in_place=True)\n",
    "        )\n",
    "        # Input: 32 * 32 * 64\n",
    "        self.disc_layer_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64,out_channels=128,kernel_size=(4,4),stride=2,padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2,in_place=True)\n",
    "        )\n",
    "        # Input: 16 * 16 * 128\n",
    "        self.disc_layer_3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128,out_channels=256,kernel_size=(4,4),stride=2,padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2,in_place=True)\n",
    "        )\n",
    "        # Input: 8 * 8 * 256\n",
    "        self.disc_layer_4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256,out_channels=512,kernel_size=(4,4),stride=2,padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2,in_place=True)\n",
    "        )\n",
    "        # Input: 4 * 4 * 512\n",
    "        self.disc_layer_5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512,out_channels=1,kernel_size=(4,4),stride=1,padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        # Output: 1 * 1 * 1\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.disc_layer_1(x)\n",
    "        x = self.disc_layer_2(x)\n",
    "        x = self.disc_layer_3(x)\n",
    "        x = self.disc_layer_4(x)\n",
    "        x = self.disc_layer_5(x)\n",
    "        x = x.view(-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all needed modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark = False \n",
    "\n",
    "# Enable cuda if available\n",
    "if torch.cuda.is_available():\n",
    "  Tensor = torch.cuda.FloatTensor\n",
    "  device = 'cuda'\n",
    "else :\n",
    "  Tensor = torch.FloatTensor\n",
    "  device = 'cpu'\n",
    "\n",
    "\n",
    "# Number of channel RGB 3\n",
    "channel = 3 \n",
    "\n",
    "# Imgage size will be C*256*256\n",
    "img_size = 256\n",
    "\n",
    "# Mask size it's the size of center mask Cx64x64\n",
    "mask_size = 64\n",
    "\n",
    "# Number of pixels overlapped\n",
    "overlapPred = 0\n",
    "\n",
    "# Size of batches\n",
    "batch_size = 64\n",
    "\n",
    "#  the lower is res value, the more continuous the output will be.\n",
    "## Value to generate a random patter of 1 and 0 to create a random region\n",
    "res = 0.06\n",
    "density = 0.25\n",
    "MAX_SIZE = 10000\n",
    "\n",
    "# Paths \n",
    "save_path_discriminator = \"./gan/checkpoint_discriminator.pth\"\n",
    "save_path_generator = \".gan/checkpoint_generator.pth\"\n",
    "\n",
    "# Restore backups\n",
    "restore = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    # Initialize model\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(bottleneck=4000, gen_lr=0.0002, dis_lr=0.0002, gen_weight_decay=5e-4, dis_weight_decay=5e-4):\n",
    "    \n",
    "    latent_dim = bottleneck\n",
    "    g_net = Generator(latent_dim)\n",
    "    d_net = Discriminator()\n",
    "    g_optimizer = optim.Adam(g_net.parameters(), lr=gen_lr, betas=(0.5,0.999), weight_decay=gen_weight_decay)\n",
    "    d_optimizer = optim.Adam(d_net.parameters(), lr=dis_lr, betas=(0.5,0.999), weight_decay=dis_weight_decay)\n",
    "    \n",
    "    g_net = g_net.to(dev)\n",
    "    d_net = d_net.to(dev)\n",
    "    return g_net,d_net,g_optimizer,d_optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(train_loader,test_loader= None, labels_noise=False,wtl2= 0.999,last_epoch=200,save_photos_interval=10,overlapL2Weight=10):\n",
    "    '''\n",
    "    train_loader: Dataloader of train data\n",
    "    test_loader : Dataloader of test data\n",
    "    labels_noise: Boolean that enable labels smoothing and flipping\n",
    "    wtl2: param to weights losses \n",
    "    last_epoch: number of last epoch\n",
    "    save_photos_interval: set interval of every x epoch generate photos to compare\n",
    "    overlapL2Weight: weights amplified \n",
    "\n",
    "    '''\n",
    "    # Define labels \n",
    "    valid = Variable(Tensor(batch_size).fill_(1.0), requires_grad=False)\n",
    "    fake = Variable(Tensor(batch_size).fill_(0.0), requires_grad=False)\n",
    "    path_toSave_photos = \"/content/images/\"\n",
    "    total_time = 0\n",
    "    # load test image\n",
    "    # test_image= next(iter(test_loader))\n",
    "    # test_image = test_image.to(dev)\n",
    "    # test_masked_imgs =test_image.clone() \n",
    "    # Create the models \n",
    "    g_net,d_net,g_optimizer,d_optimizer = create_model()\n",
    "    # If backup it's available load it \n",
    "    # if os.path.isfile(save_path_discriminator) and os.path.isfile(save_path_generator) and restore:\n",
    "    #     checkpoint_d =  torch.load(save_path_discriminator)\n",
    "    #     checkpoint_g =  torch.load(save_path_generator)\n",
    "    #     d_net.load_state_dict(checkpoint_d['d_state_dict'])\n",
    "    #     d_optimizer.load_state_dict(checkpoint_d['optimizer_state_dict'])\n",
    "    #     d_loss = checkpoint_d['loss']\n",
    "    #     d_loss_fake = checkpoint_d['loss_fake']\n",
    "    #     d_loss_real = checkpoint_d['loss_real']\n",
    "    #     g_net.load_state_dict(checkpoint_g['g_state_dict'])\n",
    "    #     g_optimizer.load_state_dict(checkpoint_g['optimizer_state_dict'])\n",
    "    #     g_loss = checkpoint_g['loss']    \n",
    "    #     g_loss_pixel = checkpoint_g['loss_pixel']\n",
    "    #     g_loss_adv = checkpoint_g['loss_adv']\n",
    "    #     epoch_backup = checkpoint_g['epoch']+1\n",
    "    #     print(\"Discriminator and Generator restored\")\n",
    "    epoch_backup = 0 \n",
    "    g_net.apply(weights_init_normal)\n",
    "    d_net.apply(weights_init_normal)\n",
    "    print(\"weight applied\")\n",
    "    try:\n",
    "        for epoch in range(epoch_backup,last_epoch):\n",
    "            # Losses\n",
    "            start = time.time()\n",
    "            sum_d_loss = 0\n",
    "            sum_d_fake_loss = 0\n",
    "            sum_d_real_loss = 0\n",
    "            sum_g_loss = 0\n",
    "            sum_g_loss_adv = 0\n",
    "            sum_g_loss_pixel = 0\n",
    "            # Training mode\n",
    "            d_net.train()\n",
    "            g_net.train()\n",
    "            # Process all training batches\n",
    "            i = 0\n",
    "    \n",
    "            for i,(x,y) in enumerate(train_loader):\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                # Move to device\n",
    "                i+=1\n",
    "#                 masked_parts = get_center(batch)\n",
    "                masked_parts = y[:,:,96:160,96:160]\n",
    "                #masked_parts are the center of the images \n",
    "                masked_parts = Variable(masked_parts.type(Tensor))\n",
    "                masked_imgs = x.clone()\n",
    "#                 masked_imgs = apply_center_mask(img_mask)\n",
    "#                 masked_imgs = Variable(masked_imgs.type(Tensor))\n",
    "\n",
    "                ### Discriminator \n",
    "                \n",
    "                # Reset discriminator gradient\n",
    "                d_optimizer.zero_grad()\n",
    "\n",
    "                # Forward (discriminator, real)\n",
    "                output = d_net(masked_parts) \n",
    "                # Compute loss (discriminator, real)\n",
    "               \n",
    "                d_real_loss =  F.binary_cross_entropy(output, valid)\n",
    "                # Backward (discriminator, real)\n",
    "                d_real_loss.backward()\n",
    "                sum_d_real_loss += d_real_loss.item()  \n",
    "                #generate sample from masked images         \n",
    "                g_output = g_net(masked_imgs)\n",
    "                # Forward (discriminator, fake; also generator forward pass)\n",
    "                output = d_net(g_output.detach()) # This prevents backpropagation from going inside the generator\n",
    "                # Compute loss (discriminator, fake)\n",
    "                d_fake_loss = F.binary_cross_entropy(output, fake)\n",
    "                # Backward (discriminator, fake)\n",
    "                d_fake_loss.backward()\n",
    "                sum_d_fake_loss += d_fake_loss.item()           \n",
    "                d_loss = 0.5*(d_fake_loss + d_real_loss)\n",
    "                sum_d_loss += d_loss.item()\n",
    "                # Update discriminator\n",
    "                d_optimizer.step()\n",
    "                \n",
    "                ### Generator \n",
    "                g_optimizer.zero_grad()\n",
    "                # Forward (generator)\n",
    "                output =  d_net(g_output)\n",
    "                # Compute adversarial loss\n",
    "                g_loss_adv = F.binary_cross_entropy(output, valid)            \n",
    "                # Comput pixelwise loss\n",
    "                # but amplifying weights 10x \n",
    "                #g_loss_pixel =  criterionMSE(g_output,masked_parts)\n",
    "#                 wtl2Matrix = masked_parts.clone()\n",
    "                # OverlapL2weight = 10\n",
    "#                 wtl2Matrix.data.fill_(wtl2*overlapL2Weight)\n",
    "#                wtl2Matrix.data[:,:,overlapPred:mask_size-overlapPred,overlapPred:mask_size-overlapPred] = wtl2\n",
    "                # MSE Loss\n",
    "                g_loss_pixel = (g_output-masked_parts).pow(2)\n",
    "                # Multiply \n",
    "#                 g_loss_pixel = g_loss_pixel * wtl2Matrix\n",
    "                g_loss_pixel = g_loss_pixel.mean()\n",
    "                # The losse it's the sum of adv and pixel\n",
    "                g_loss = (1-wtl2) * g_loss_adv + wtl2 * g_loss_pixel\n",
    "                sum_g_loss_adv += g_loss_adv.item()\n",
    "                sum_g_loss_pixel += g_loss_pixel.item()\n",
    "                sum_g_loss += g_loss.item()\n",
    "                # Backward (generator)\n",
    "                g_loss.backward()\n",
    "                # Update generator\n",
    "                g_optimizer.step()\n",
    "                if epoch % 2 == 0:\n",
    "                    torch.save({'g_state_dict': g_net.state_dict(),\n",
    "                            'optimizer_state_dict': g_optimizer.state_dict(),\n",
    "                            'loss': g_loss,\n",
    "                            'loss_pixel': g_loss_pixel,\n",
    "                            'loss_adv': g_loss_adv,\n",
    "                            'epoch':epoch,\n",
    "                            }, save_path_generator)\n",
    "\n",
    "                    torch.save({'d_state_dict': d_net.state_dict(),\n",
    "                                'optimizer_state_dict': d_optimizer.state_dict(),\n",
    "                                'loss_fake': d_fake_loss,\n",
    "                                'loss_real': d_real_loss,\n",
    "                                'loss': d_loss,\n",
    "                                'epoch': epoch\n",
    "                                }, save_path_discriminator)\n",
    "#                 if (i%700==0):\n",
    "#                     print(f\"Batches {i}/{len(train_loader)}\")\n",
    "\n",
    "            # Epoch end, print losses\n",
    "            epoch_d_loss = sum_d_loss/len(train_loader)\n",
    "            epoch_d_real_loss = sum_d_real_loss/len(train_loader)\n",
    "            epoch_d_fake_loss = sum_d_fake_loss/len(train_loader)\n",
    "            epoch_g_loss_adv = sum_g_loss_adv/len(train_loader)\n",
    "            epoch_g_loss_pixel = sum_g_loss_pixel/len(train_loader)\n",
    "            epoch_g_loss = sum_g_loss/len(train_loader)\n",
    "            end = time.time()   \n",
    "            time_epoch = (end - start)/60\n",
    "            total_time +=time_epoch\n",
    "            # Save models\n",
    "#             torch.save({'g_state_dict': g_net.state_dict(),\n",
    "#                         'optimizer_state_dict': g_optimizer.state_dict(),\n",
    "#                         'loss': g_loss,\n",
    "#                         'loss_pixel': g_loss_pixel,\n",
    "#                         'loss_adv': g_loss_adv,\n",
    "#                         'epoch':epoch,\n",
    "#                         }, save_path_generator)\n",
    "\n",
    "#             torch.save({'d_state_dict': d_net.state_dict(),\n",
    "#                         'optimizer_state_dict': d_optimizer.state_dict(),\n",
    "#                         'loss_fake': d_fake_loss,\n",
    "#                         'loss_real': d_real_loss,\n",
    "#                         'loss': d_loss,\n",
    "#                         'epoch': epoch\n",
    "#                         }, save_path_discriminator)\n",
    "#             if ((epoch+1)%save_photos_interval==0):\n",
    "#                 compare_and_save(64,path_toSave_photos,test_loader,g_net)\n",
    "#             print(f\"Epoch {epoch+1} DL={epoch_d_loss:.4f} DR={epoch_d_real_loss:.4f} DF={epoch_d_fake_loss:.4f} GL={epoch_g_loss:.4f} GLP={epoch_g_loss_pixel:.4f} GLADV={epoch_g_loss_adv:.4f} Time {time_epoch:.1f}min Total Time: {total_time/60 :.1f}h\")\n",
    "#             # Evaluation mode\n",
    "#             g_net.eval()\n",
    "#             with torch.no_grad():\n",
    "#                 # Removing center from the test sample\n",
    "#                 sample = apply_center_mask(test_image)\n",
    "#                 # Forward (generator)\n",
    "#                 g_sample = g_net(sample)\n",
    "#                 # Impanting the image generated to the original\n",
    "#                 test_masked_imgs[:,:,(mask_size//2):img_size-(mask_size//2),(mask_size//2):img_size-(mask_size//2)] = g_sample.data\n",
    "#                 plt.imshow(TF.to_pil_image(make_grid(test_masked_imgs[:4], scale_each=True, normalize=True).cpu()))\n",
    "#                 plt.axis('off')\n",
    "#                 plt.show()\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "          print(\"Interrupted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training(train_loader)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
